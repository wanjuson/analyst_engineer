softm = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', C=10)
softm.fit(train_x, train_y)

(2) 서포트 벡터 분류기
 모든 데이터가 초평면에 의해 두 영역으로 분류될 수 있는 것이 아니기 때문에, 최대 마진 분류기는 모든 데이터에 적용하기 어렵다는 단점이 있다. 또한 분리 초평면에 의해 데이터 클래스를 나누면 학습데이터를 완벽하게 분류하기 때문에 과적합의 문제와 테스트데이터의 이상치에 민감할 수 있다는 문제도 있다. 그러므로 이상치로부터 영향을 덜 받으면서도 대부분의 학습데이터를 잘 분류할 수 있는 방식이 필요했다. 이를 위해 서포트 벡터 분류기는 최대 마진 분류기를 가지면서도 일부 관측치들이 마진이나 초평면의 반대쪽에 있는 것을 허용한다.
SVC는 오차항을 수용하기 위해 제약조건을 아래와 같이 허용한다.

SVM 분류기는 일정한 마진 오류 안에서 두 클래스간의 도로 폭이 최대가 되도록 했지만 SVR은 아래 그림처럼 제한된 마진 오류 안에서 도로 안에 가능한 많은 데이터 샘플이 들어가도록 학습한다. 이때, 마진오류는 도로 밖의 데이터 샘플을 의미한다.
SVR에서 사용하는 손실 함수 중 가장 대표적인 것은 epsilon-insensitive 함수이다. epsilon-insensitive 함수를 사용한 SVR식은 다음과 같다.
수식에 사용된 하이퍼파라미터에 대한 설명은 다음과 같다. C(Cost)는 비용을 의마하며 패널티항을 통해 규제의 정도를 결정짓는다. C는 작아질수록 잘못 예측한 값에 대해 패널티를 부여하기 때문에 C가 작아질수록 회귀식이 평평해진다. e는 회귀식 위아래로 혀용하는 노이즈의 저옫이다. e가 클수록 허용 노이즈가 많아진다. 

# SVM은 특성의 스케일에 민감하기 때문에 scikit-learn의 StandardScaler를 사용하면 좀더 예측력 높은 결정경계를 생성할 수 있다.
StandardScaler를 이용해 스케일링을 한 이후 산점도를 그러보면 클래스의 특성이 더 잘 시각화되는 것을 확인할 수 있다.
SVC로 모델을 만들고 데이터를 적합해보자.
