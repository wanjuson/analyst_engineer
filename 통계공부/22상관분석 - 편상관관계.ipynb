{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22상관분석 - 편상관관계\n",
    "# https://www.youtube.com/watch?v=LKK9PaSu9zU&list=PLY0OaF78qqGAxKX91WuRigHpwBU0C2SB_&index=23\n",
    "# https://www.youtube.com/watch?v=PfGwTtRZdSU&list=PLCt8K88AxcKPucYITFaLshFeCIp8F9IGk&index=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088be6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅁ 편상관계수\n",
    "#  - 두 변수 간의 순수한 상관관계\n",
    "#  - 하나 이상의 다른 변수의 영향을 통제한 상태에서 관심의 대상인 두 변수간의 선형적 관련성을 측정\n",
    "#   . 가짜 상관을 찾아내느데 활용(예 : 연봉과 혈압 ~ 나이)\n",
    "#   . 숨겨진 관계를 찾는데 활용(예 : 구매필요성과 구매의향 ~ 소득)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d13dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32 entries, 0 to 31\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  32 non-null     object \n",
      " 1   mpg         32 non-null     float64\n",
      " 2   cyl         32 non-null     int64  \n",
      " 3   disp        32 non-null     float64\n",
      " 4   hp          32 non-null     int64  \n",
      " 5   drat        32 non-null     float64\n",
      " 6   wt          32 non-null     float64\n",
      " 7   qsec        32 non-null     float64\n",
      " 8   vs          32 non-null     int64  \n",
      " 9   am          32 non-null     int64  \n",
      " 10  gear        32 non-null     int64  \n",
      " 11  carb        32 non-null     int64  \n",
      "dtypes: float64(5), int64(6), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/adp/data/mtcars.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c8c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>hp</th>\n",
       "      <th>wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mpg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.852162</td>\n",
       "      <td>-0.776168</td>\n",
       "      <td>-0.867659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyl</th>\n",
       "      <td>-0.852162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832447</td>\n",
       "      <td>0.782496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.776168</td>\n",
       "      <td>0.832447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.658748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wt</th>\n",
       "      <td>-0.867659</td>\n",
       "      <td>0.782496</td>\n",
       "      <td>0.658748</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mpg       cyl        hp        wt\n",
       "mpg  1.000000 -0.852162 -0.776168 -0.867659\n",
       "cyl -0.852162  1.000000  0.832447  0.782496\n",
       "hp  -0.776168  0.832447  1.000000  0.658748\n",
       "wt  -0.867659  0.782496  0.658748  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['mpg', 'cyl', 'hp', 'wt']]\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "431328fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>r</th>\n",
       "      <th>CI95%</th>\n",
       "      <th>p-val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pearson</th>\n",
       "      <td>32</td>\n",
       "      <td>-0.275893</td>\n",
       "      <td>[-0.58, 0.09]</td>\n",
       "      <td>0.140015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          n         r          CI95%     p-val\n",
       "pearson  32 -0.275893  [-0.58, 0.09]  0.140015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pingouin as pg\n",
    "pg.partial_corr(data = df, x = 'mpg', y='hp', covar = ['cyl', 'wt'])\n",
    "\n",
    "# 'mpg', 'hp' 상관계수는 -0.776168, 편상관계수는 -0.275893 으로 축소되어 ['cyl', 'wt'] 영향이 있는것으로 보임\n",
    "# p-val 0.140015로 유의수준 0.05하에서 0.05보다 크므로 ['cyl', 'wt']를 통제하므로 'mpg', 'hp' 상관관계는 없다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538278d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pingouin.correlation in pingouin:\n",
      "\n",
      "NAME\n",
      "    pingouin.correlation - # Author: Raphael Vallat <raphaelvallat9@gmail.com>\n",
      "\n",
      "FUNCTIONS\n",
      "    corr(x, y, alternative='two-sided', method='pearson', **kwargs)\n",
      "        (Robust) correlation between two variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            First and second set of observations. ``x`` and ``y`` must be\n",
      "            independent.\n",
      "        alternative : string\n",
      "            Defines the alternative hypothesis, or tail of the correlation. Must be one of\n",
      "            \"two-sided\" (default), \"greater\" or \"less\". Both \"greater\" and \"less\" return a one-sided\n",
      "            p-value. \"greater\" tests against the alternative hypothesis that the correlation is\n",
      "            positive (greater than zero), \"less\" tests against the hypothesis that the correlation is\n",
      "            negative.\n",
      "        method : string\n",
      "            Correlation type:\n",
      "        \n",
      "            * ``'pearson'``: Pearson :math:`r` product-moment correlation\n",
      "            * ``'spearman'``: Spearman :math:`\\rho` rank-order correlation\n",
      "            * ``'kendall'``: Kendall's :math:`\\tau_B` correlation (for ordinal data)\n",
      "            * ``'bicor'``: Biweight midcorrelation (robust)\n",
      "            * ``'percbend'``: Percentage bend correlation (robust)\n",
      "            * ``'shepherd'``: Shepherd's pi correlation (robust)\n",
      "            * ``'skipped'``: Skipped correlation (robust)\n",
      "        **kwargs : optional\n",
      "            Optional argument(s) passed to the lower-level correlation functions.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stats : :py:class:`pandas.DataFrame`\n",
      "        \n",
      "            * ``'n'``: Sample size (after removal of missing values)\n",
      "            * ``'outliers'``: number of outliers, only if a robust method was used\n",
      "            * ``'r'``: Correlation coefficient\n",
      "            * ``'CI95'``: 95% parametric confidence intervals around :math:`r`\n",
      "            * ``'p-val'``: p-value\n",
      "            * ``'BF10'``: Bayes Factor of the alternative hypothesis (only for Pearson correlation)\n",
      "            * ``'power'``: achieved power of the test with an alpha of 0.05.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        pairwise_corr : Pairwise correlation between columns of a pandas DataFrame\n",
      "        partial_corr : Partial correlation\n",
      "        rm_corr : Repeated measures correlation\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `Pearson correlation coefficient\n",
      "        <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n",
      "        measures the linear relationship between two datasets. Strictly speaking,\n",
      "        Pearson's correlation requires that each dataset be normally distributed.\n",
      "        Correlations of -1 or +1 imply a perfect negative and positive linear\n",
      "        relationship, respectively, with 0 indicating the absence of association.\n",
      "        \n",
      "        .. math::\n",
      "            r_{xy} = \\frac{\\sum_i(x_i - \\bar{x})(y_i - \\bar{y})}\n",
      "            {\\sqrt{\\sum_i(x_i - \\bar{x})^2} \\sqrt{\\sum_i(y_i - \\bar{y})^2}}\n",
      "            = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
      "        \n",
      "        where :math:`\\text{cov}` is the sample covariance and :math:`\\sigma`\n",
      "        is the sample standard deviation.\n",
      "        \n",
      "        If ``method='pearson'``, The Bayes Factor is calculated using the\n",
      "        :py:func:`pingouin.bayesfactor_pearson` function.\n",
      "        \n",
      "        The `Spearman correlation coefficient\n",
      "        <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n",
      "        is a non-parametric measure of the monotonicity of the relationship between\n",
      "        two datasets. Unlike the Pearson correlation, the Spearman correlation does\n",
      "        not assume that both datasets are normally distributed. Correlations of -1\n",
      "        or +1 imply an exact negative and positive monotonic relationship,\n",
      "        respectively. Mathematically, the Spearman correlation coefficient is\n",
      "        defined as the Pearson correlation coefficient between the\n",
      "        `rank variables <https://en.wikipedia.org/wiki/Ranking>`_.\n",
      "        \n",
      "        The `Kendall correlation coefficient\n",
      "        <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n",
      "        is a measure of the correspondence between two rankings. Values also range\n",
      "        from -1 (perfect disagreement) to 1 (perfect agreement), with 0 indicating\n",
      "        the absence of association. Consistent with\n",
      "        :py:func:`scipy.stats.kendalltau`, Pingouin returns the Tau-b coefficient,\n",
      "        which adjusts for ties:\n",
      "        \n",
      "        .. math:: \\tau_B = \\frac{(P - Q)}{\\sqrt{(P + Q + T) (P + Q + U)}}\n",
      "        \n",
      "        where :math:`P` is the number of concordant pairs, :math:`Q` the number of\n",
      "        discordand pairs, :math:`T` the number of ties in x, and :math:`U`\n",
      "        the number of ties in y.\n",
      "        \n",
      "        The `biweight midcorrelation\n",
      "        <https://en.wikipedia.org/wiki/Biweight_midcorrelation>`_ and\n",
      "        percentage bend correlation [1]_ are both robust methods that\n",
      "        protects against *univariate* outliers by down-weighting observations that\n",
      "        deviate too much from the median.\n",
      "        \n",
      "        The Shepherd pi [2]_ correlation and skipped [3]_, [4]_ correlation are\n",
      "        both robust methods that returns the Spearman correlation coefficient after\n",
      "        removing *bivariate* outliers. Briefly, the Shepherd pi uses a\n",
      "        bootstrapping of the Mahalanobis distance to identify outliers, while the\n",
      "        skipped correlation is based on the minimum covariance determinant\n",
      "        (which requires scikit-learn). Note that these two methods are\n",
      "        significantly slower than the previous ones.\n",
      "        \n",
      "        The confidence intervals for the correlation coefficient are estimated\n",
      "        using the Fisher transformation.\n",
      "        \n",
      "        .. important:: Rows with missing values (NaN) are automatically removed.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wilcox, R.R., 1994. The percentage bend correlation coefficient.\n",
      "           Psychometrika 59, 601–616. https://doi.org/10.1007/BF02294395\n",
      "        \n",
      "        .. [2] Schwarzkopf, D.S., De Haas, B., Rees, G., 2012. Better ways to\n",
      "           improve standards in brain-behavior correlation analysis. Front.\n",
      "           Hum. Neurosci. 6, 200. https://doi.org/10.3389/fnhum.2012.00200\n",
      "        \n",
      "        .. [3] Rousselet, G.A., Pernet, C.R., 2012. Improving standards in\n",
      "           brain-behavior correlation analyses. Front. Hum. Neurosci. 6, 119.\n",
      "           https://doi.org/10.3389/fnhum.2012.00119\n",
      "        \n",
      "        .. [4] Pernet, C.R., Wilcox, R., Rousselet, G.A., 2012. Robust correlation\n",
      "           analyses: false positive and power validation using a new open\n",
      "           source matlab toolbox. Front. Psychol. 3, 606.\n",
      "           https://doi.org/10.3389/fpsyg.2012.00606\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        1. Pearson correlation\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> import pingouin as pg\n",
      "        >>> # Generate random correlated samples\n",
      "        >>> np.random.seed(123)\n",
      "        >>> mean, cov = [4, 6], [(1, .5), (.5, 1)]\n",
      "        >>> x, y = np.random.multivariate_normal(mean, cov, 30).T\n",
      "        >>> # Compute Pearson correlation\n",
      "        >>> pg.corr(x, y).round(3)\n",
      "                  n      r         CI95%  p-val  BF10  power\n",
      "        pearson  30  0.491  [0.16, 0.72]  0.006  8.55  0.809\n",
      "        \n",
      "        2. Pearson correlation with two outliers\n",
      "        \n",
      "        >>> x[3], y[5] = 12, -8\n",
      "        >>> pg.corr(x, y).round(3)\n",
      "                  n      r          CI95%  p-val   BF10  power\n",
      "        pearson  30  0.147  [-0.23, 0.48]  0.439  0.302  0.121\n",
      "        \n",
      "        3. Spearman correlation (robust to outliers)\n",
      "        \n",
      "        >>> pg.corr(x, y, method=\"spearman\").round(3)\n",
      "                   n      r         CI95%  p-val  power\n",
      "        spearman  30  0.401  [0.05, 0.67]  0.028   0.61\n",
      "        \n",
      "        4. Biweight midcorrelation (robust)\n",
      "        \n",
      "        >>> pg.corr(x, y, method=\"bicor\").round(3)\n",
      "                n      r         CI95%  p-val  power\n",
      "        bicor  30  0.393  [0.04, 0.66]  0.031  0.592\n",
      "        \n",
      "        5. Percentage bend correlation (robust)\n",
      "        \n",
      "        >>> pg.corr(x, y, method='percbend').round(3)\n",
      "                   n      r         CI95%  p-val  power\n",
      "        percbend  30  0.389  [0.03, 0.66]  0.034  0.581\n",
      "        \n",
      "        6. Shepherd's pi correlation (robust)\n",
      "        \n",
      "        >>> pg.corr(x, y, method='shepherd').round(3)\n",
      "                   n  outliers      r        CI95%  p-val  power\n",
      "        shepherd  30         2  0.437  [0.08, 0.7]   0.02  0.662\n",
      "        \n",
      "        7. Skipped spearman correlation (robust)\n",
      "        \n",
      "        >>> pg.corr(x, y, method='skipped').round(3)\n",
      "                  n  outliers      r        CI95%  p-val  power\n",
      "        skipped  30         2  0.437  [0.08, 0.7]   0.02  0.662\n",
      "        \n",
      "        8. One-tailed Pearson correlation\n",
      "        \n",
      "        >>> pg.corr(x, y, alternative=\"greater\", method='pearson').round(3)\n",
      "                n      r           CI95%  p-val   BF10  power\n",
      "        pearson  30  0.147  [-0.17, 1.0]   0.22  0.467  0.194\n",
      "        \n",
      "        >>> pg.corr(x, y, alternative=\"less\", method='pearson').round(3)\n",
      "                n        r         CI95%  p-val   BF10  power\n",
      "        pearson  30  0.147  [-1.0, 0.43]   0.78  0.137  0.008\n",
      "        \n",
      "        9. Perfect correlation\n",
      "        \n",
      "        >>> pg.corr(x, -x).round(3)\n",
      "                  n    r         CI95%  p-val BF10  power\n",
      "        pearson  30 -1.0  [-1.0, -1.0]    0.0  inf      1\n",
      "        \n",
      "        10. Using columns of a pandas dataframe\n",
      "        \n",
      "        >>> import pandas as pd\n",
      "        >>> data = pd.DataFrame({'x': x, 'y': y})\n",
      "        >>> pg.corr(data['x'], data['y']).round(3)\n",
      "                  n      r          CI95%  p-val   BF10  power\n",
      "        pearson  30  0.147  [-0.23, 0.48]  0.439  0.302  0.121\n",
      "    \n",
      "    distance_corr(x, y, alternative='greater', n_boot=1000, seed=None)\n",
      "        Distance correlation between two arrays.\n",
      "        \n",
      "        Statistical significance (p-value) is evaluated with a permutation test.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            1D or 2D input arrays, shape (n_samples, n_features).\n",
      "            ``x`` and ``y`` must have the same number of samples and must not\n",
      "            contain missing values.\n",
      "        alternative : str\n",
      "            Alternative of the test. Can be either \"two-sided\", \"greater\" (default) or \"less\".\n",
      "            To be consistent with the original R implementation, the default is to calculate the\n",
      "            one-sided \"greater\" p-value.\n",
      "        n_boot : int or None\n",
      "            Number of bootstrap to perform. If None, no bootstrapping is performed and the function\n",
      "            only returns the distance correlation (no p-value). Default is 1000 (thus giving a\n",
      "            precision of 0.001).\n",
      "        seed : int or None\n",
      "            Random state seed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dcor : float\n",
      "            Sample distance correlation (range from 0 to 1).\n",
      "        pval : float\n",
      "            P-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        From Wikipedia:\n",
      "        \n",
      "            *Distance correlation is a measure of dependence between two paired\n",
      "            random vectors of arbitrary, not necessarily equal, dimension. The\n",
      "            distance correlation coefficient is zero if and only if the random\n",
      "            vectors are independent. Thus, distance correlation measures both\n",
      "            linear and nonlinear association between two random variables or\n",
      "            random vectors. This is in contrast to Pearson's correlation, which can\n",
      "            only detect linear association between two random variables.*\n",
      "        \n",
      "        The distance correlation of two random variables is obtained by\n",
      "        dividing their distance covariance by the product of their distance\n",
      "        standard deviations:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\text{dCor}(X, Y) = \\frac{\\text{dCov}(X, Y)}\n",
      "            {\\sqrt{\\text{dVar}(X) \\cdot \\text{dVar}(Y)}}\n",
      "        \n",
      "        where :math:`\\text{dCov}(X, Y)` is the square root of the arithmetic\n",
      "        average of the product of the double-centered pairwise Euclidean distance\n",
      "        matrices.\n",
      "        \n",
      "        Note that by contrast to Pearson's correlation, the distance correlation\n",
      "        cannot be negative, i.e :math:`0 \\leq \\text{dCor} \\leq 1`.\n",
      "        \n",
      "        Results have been tested against the\n",
      "        `energy <https://cran.r-project.org/web/packages/energy/energy.pdf>`_\n",
      "        R package.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * https://en.wikipedia.org/wiki/Distance_correlation\n",
      "        \n",
      "        * Székely, G. J., Rizzo, M. L., & Bakirov, N. K. (2007).\n",
      "          Measuring and testing dependence by correlation of distances.\n",
      "          The annals of statistics, 35(6), 2769-2794.\n",
      "        \n",
      "        * https://gist.github.com/satra/aa3d19a12b74e9ab7941\n",
      "        \n",
      "        * https://gist.github.com/wladston/c931b1495184fbb99bec\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        1. With two 1D vectors\n",
      "        \n",
      "        >>> from pingouin import distance_corr\n",
      "        >>> a = [1, 2, 3, 4, 5]\n",
      "        >>> b = [1, 2, 9, 4, 4]\n",
      "        >>> dcor, pval = distance_corr(a, b, seed=9)\n",
      "        >>> print(round(dcor, 3), pval)\n",
      "        0.763 0.312\n",
      "        \n",
      "        2. With two 2D arrays and no p-value\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> np.random.seed(123)\n",
      "        >>> from pingouin import distance_corr\n",
      "        >>> a = np.random.random((10, 10))\n",
      "        >>> b = np.random.random((10, 10))\n",
      "        >>> round(distance_corr(a, b, n_boot=None), 3)\n",
      "        0.88\n",
      "    \n",
      "    partial_corr(data=None, x=None, y=None, covar=None, x_covar=None, y_covar=None, alternative='two-sided', method='pearson')\n",
      "        Partial and semi-partial correlation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : :py:class:`pandas.DataFrame`\n",
      "            Pandas Dataframe. Note that this function can also directly be used\n",
      "            as a :py:class:`pandas.DataFrame` method, in which case this argument\n",
      "            is no longer needed.\n",
      "        x, y : string\n",
      "            x and y. Must be names of columns in ``data``.\n",
      "        covar : string or list\n",
      "            Covariate(s). Must be a names of columns in ``data``. Use a list if\n",
      "            there are two or more covariates.\n",
      "        x_covar : string or list\n",
      "            Covariate(s) for the ``x`` variable. This is used to compute\n",
      "            semi-partial correlation (i.e. the effect of ``x_covar`` is removed\n",
      "            from ``x`` but not from ``y``). Only one of ``covar``,  ``x_covar`` and\n",
      "            ``y_covar`` can be specified.\n",
      "        y_covar : string or list\n",
      "            Covariate(s) for the ``y`` variable. This is used to compute\n",
      "            semi-partial correlation (i.e. the effect of ``y_covar`` is removed\n",
      "            from ``y`` but not from ``x``). Only one of ``covar``,  ``x_covar`` and\n",
      "            ``y_covar`` can be specified.\n",
      "        alternative : string\n",
      "            Defines the alternative hypothesis, or tail of the partial correlation. Must be one of\n",
      "            \"two-sided\" (default), \"greater\" or \"less\". Both \"greater\" and \"less\" return a one-sided\n",
      "            p-value. \"greater\" tests against the alternative hypothesis that the partial correlation is\n",
      "            positive (greater than zero), \"less\" tests against the hypothesis that the partial\n",
      "            correlation is negative.\n",
      "        method : string\n",
      "            Correlation type:\n",
      "        \n",
      "            * ``'pearson'``: Pearson :math:`r` product-moment correlation\n",
      "            * ``'spearman'``: Spearman :math:`\\rho` rank-order correlation\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stats : :py:class:`pandas.DataFrame`\n",
      "        \n",
      "            * ``'n'``: Sample size (after removal of missing values)\n",
      "            * ``'r'``: Partial correlation coefficient\n",
      "            * ``'CI95'``: 95% parametric confidence intervals around :math:`r`\n",
      "            * ``'p-val'``: p-value\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        corr, pcorr, pairwise_corr, rm_corr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Partial correlation [1]_ measures the degree of association between ``x``\n",
      "        and ``y``, after removing the effect of one or more controlling variables\n",
      "        (``covar``, or :math:`Z`). Practically, this is achieved by calculating the\n",
      "        correlation coefficient between the residuals of two linear regressions:\n",
      "        \n",
      "        .. math:: x \\sim Z, y \\sim Z\n",
      "        \n",
      "        Like the correlation coefficient, the partial correlation\n",
      "        coefficient takes on a value in the range from –1 to 1, where 1 indicates a\n",
      "        perfect positive association.\n",
      "        \n",
      "        The semipartial correlation is similar to the partial correlation,\n",
      "        with the exception that the set of controlling variables is only\n",
      "        removed for either ``x`` or ``y``, but not both.\n",
      "        \n",
      "        Pingouin uses the method described in [2]_ to calculate the (semi)partial\n",
      "        correlation coefficients and associated p-values. This method is based on\n",
      "        the inverse covariance matrix and is significantly faster than the\n",
      "        traditional regression-based method. Results have been tested against the\n",
      "        `ppcor <https://cran.r-project.org/web/packages/ppcor/index.html>`_\n",
      "        R package.\n",
      "        \n",
      "        .. important:: Rows with missing values are automatically removed from\n",
      "            data.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Partial_correlation\n",
      "        \n",
      "        .. [2] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4681537/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        1. Partial correlation with one covariate\n",
      "        \n",
      "        >>> import pingouin as pg\n",
      "        >>> df = pg.read_dataset('partial_corr')\n",
      "        >>> pg.partial_corr(data=df, x='x', y='y', covar='cv1').round(3)\n",
      "                  n      r         CI95%  p-val\n",
      "        pearson  30  0.568  [0.25, 0.77]  0.001\n",
      "        \n",
      "        2. Spearman partial correlation with several covariates\n",
      "        \n",
      "        >>> # Partial correlation of x and y controlling for cv1, cv2 and cv3\n",
      "        >>> pg.partial_corr(data=df, x='x', y='y', covar=['cv1', 'cv2', 'cv3'],\n",
      "        ...                 method='spearman').round(3)\n",
      "                   n      r         CI95%  p-val\n",
      "        spearman  30  0.521  [0.18, 0.75]  0.005\n",
      "        \n",
      "        3. Same but one-sided test\n",
      "        \n",
      "        >>> pg.partial_corr(data=df, x='x', y='y', covar=['cv1', 'cv2', 'cv3'],\n",
      "        ...                 alternative=\"greater\", method='spearman').round(3)\n",
      "                   n      r        CI95%  p-val\n",
      "        spearman  30  0.521  [0.24, 1.0]  0.003\n",
      "        \n",
      "        >>> pg.partial_corr(data=df, x='x', y='y', covar=['cv1', 'cv2', 'cv3'],\n",
      "        ...                 alternative=\"less\", method='spearman').round(3)\n",
      "                   n      r         CI95%  p-val\n",
      "        spearman  30  0.521  [-1.0, 0.72]  0.997\n",
      "        \n",
      "        4. As a pandas method\n",
      "        \n",
      "        >>> df.partial_corr(x='x', y='y', covar=['cv1'], method='spearman').round(3)\n",
      "                   n      r         CI95%  p-val\n",
      "        spearman  30  0.578  [0.27, 0.78]  0.001\n",
      "        \n",
      "        5. Partial correlation matrix (returns only the correlation coefficients)\n",
      "        \n",
      "        >>> df.pcorr().round(3)\n",
      "                 x      y    cv1    cv2    cv3\n",
      "        x    1.000  0.493 -0.095  0.130 -0.385\n",
      "        y    0.493  1.000 -0.007  0.104 -0.002\n",
      "        cv1 -0.095 -0.007  1.000 -0.241 -0.470\n",
      "        cv2  0.130  0.104 -0.241  1.000 -0.118\n",
      "        cv3 -0.385 -0.002 -0.470 -0.118  1.000\n",
      "        \n",
      "        6. Semi-partial correlation on x\n",
      "        \n",
      "        >>> pg.partial_corr(data=df, x='x', y='y', x_covar=['cv1', 'cv2', 'cv3']).round(3)\n",
      "                  n      r        CI95%  p-val\n",
      "        pearson  30  0.463  [0.1, 0.72]  0.015\n",
      "    \n",
      "    pcorr(self)\n",
      "        Partial correlation matrix (:py:class:`pandas.DataFrame` method).\n",
      "        \n",
      "        Returns\n",
      "        ----------\n",
      "        pcormat : :py:class:`pandas.DataFrame`\n",
      "            Partial correlation matrix.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function calculates the pairwise partial correlations for each pair of\n",
      "        variables in a :py:class:`pandas.DataFrame` given all the others. It has\n",
      "        the same behavior as the pcor function in the\n",
      "        `ppcor <https://cran.r-project.org/web/packages/ppcor/index.html>`_\n",
      "        R package.\n",
      "        \n",
      "        Note that this function only returns the raw Pearson correlation\n",
      "        coefficient. If you want to calculate the test statistic and p-values, or\n",
      "        use more robust estimates of the correlation coefficient, please refer to\n",
      "        the :py:func:`pingouin.pairwise_corr` or :py:func:`pingouin.partial_corr`\n",
      "        functions.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pingouin as pg\n",
      "        >>> data = pg.read_dataset('mediation')\n",
      "        >>> data.pcorr().round(3)\n",
      "                  X      M      Y   Mbin   Ybin     W1     W2\n",
      "        X     1.000  0.359  0.074 -0.019 -0.147 -0.148 -0.067\n",
      "        M     0.359  1.000  0.555 -0.024 -0.112 -0.138 -0.176\n",
      "        Y     0.074  0.555  1.000 -0.001  0.169  0.101  0.108\n",
      "        Mbin -0.019 -0.024 -0.001  1.000 -0.080 -0.032 -0.040\n",
      "        Ybin -0.147 -0.112  0.169 -0.080  1.000 -0.000 -0.140\n",
      "        W1   -0.148 -0.138  0.101 -0.032 -0.000  1.000 -0.394\n",
      "        W2   -0.067 -0.176  0.108 -0.040 -0.140 -0.394  1.000\n",
      "        \n",
      "        On a subset of columns\n",
      "        \n",
      "        >>> data[['X', 'Y', 'M']].pcorr()\n",
      "                  X         Y         M\n",
      "        X  1.000000  0.036649  0.412804\n",
      "        Y  0.036649  1.000000  0.540140\n",
      "        M  0.412804  0.540140  1.000000\n",
      "    \n",
      "    rcorr(self, method='pearson', upper='pval', decimals=3, padjust=None, stars=True, pval_stars={0.001: '***', 0.01: '**', 0.05: '*'})\n",
      "        Correlation matrix of a dataframe with p-values and/or sample size on the\n",
      "        upper triangle (:py:class:`pandas.DataFrame` method).\n",
      "        \n",
      "        This method is a faster, but less exhaustive, matrix-version of the\n",
      "        :py:func:`pingouin.pairwise_corr` function. It is based on the\n",
      "        :py:func:`pandas.DataFrame.corr` method. Missing values are automatically\n",
      "        removed from each pairwise correlation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        self : :py:class:`pandas.DataFrame`\n",
      "            Input dataframe.\n",
      "        method : str\n",
      "            Correlation method. Can be either 'pearson' or 'spearman'.\n",
      "        upper : str\n",
      "            If 'pval', the upper triangle of the output correlation matrix shows\n",
      "            the p-values. If 'n', the upper triangle is the sample size used in\n",
      "            each pairwise correlation.\n",
      "        decimals : int\n",
      "            Number of decimals to display in the output correlation matrix.\n",
      "        padjust : string or None\n",
      "            Method used for testing and adjustment of pvalues.\n",
      "        \n",
      "            * ``'none'``: no correction\n",
      "            * ``'bonf'``: one-step Bonferroni correction\n",
      "            * ``'sidak'``: one-step Sidak correction\n",
      "            * ``'holm'``: step-down method using Bonferroni adjustments\n",
      "            * ``'fdr_bh'``: Benjamini/Hochberg FDR correction\n",
      "            * ``'fdr_by'``: Benjamini/Yekutieli FDR correction\n",
      "        stars : boolean\n",
      "            If True, only significant p-values are displayed as stars using the\n",
      "            pre-defined thresholds of ``pval_stars``. If False, all the raw\n",
      "            p-values are displayed.\n",
      "        pval_stars : dict\n",
      "            Significance thresholds. Default is 3 stars for p-values < 0.001,\n",
      "            2 stars for p-values < 0.01 and 1 star for p-values < 0.05.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rcorr : :py:class:`pandas.DataFrame`\n",
      "            Correlation matrix, of type str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> import pandas as pd\n",
      "        >>> import pingouin as pg\n",
      "        >>> # Load an example dataset of personality dimensions\n",
      "        >>> df = pg.read_dataset('pairwise_corr').iloc[:, 1:]\n",
      "        >>> # Add some missing values\n",
      "        >>> df.iloc[[2, 5, 20], 2] = np.nan\n",
      "        >>> df.iloc[[1, 4, 10], 3] = np.nan\n",
      "        >>> df.head().round(2)\n",
      "           Neuroticism  Extraversion  Openness  Agreeableness  Conscientiousness\n",
      "        0         2.48          4.21      3.94           3.96               3.46\n",
      "        1         2.60          3.19      3.96            NaN               3.23\n",
      "        2         2.81          2.90       NaN           2.75               3.50\n",
      "        3         2.90          3.56      3.52           3.17               2.79\n",
      "        4         3.02          3.33      4.02            NaN               2.85\n",
      "        \n",
      "        >>> # Correlation matrix on the four first columns\n",
      "        >>> df.iloc[:, 0:4].rcorr()\n",
      "                      Neuroticism Extraversion Openness Agreeableness\n",
      "        Neuroticism             -          ***                     **\n",
      "        Extraversion        -0.35            -      ***\n",
      "        Openness            -0.01        0.265        -           ***\n",
      "        Agreeableness      -0.134        0.054    0.161             -\n",
      "        \n",
      "        >>> # Spearman correlation and Holm adjustement for multiple comparisons\n",
      "        >>> df.iloc[:, 0:4].rcorr(method='spearman', padjust='holm')\n",
      "                      Neuroticism Extraversion Openness Agreeableness\n",
      "        Neuroticism             -          ***                     **\n",
      "        Extraversion       -0.325            -      ***\n",
      "        Openness           -0.027         0.24        -           ***\n",
      "        Agreeableness       -0.15         0.06    0.173             -\n",
      "        \n",
      "        >>> # Compare with the pg.pairwise_corr function\n",
      "        >>> pairwise = df.iloc[:, 0:4].pairwise_corr(method='spearman',\n",
      "        ...                                          padjust='holm')\n",
      "        >>> pairwise[['X', 'Y', 'r', 'p-corr']].round(3)  # Do not show all columns\n",
      "                      X              Y      r  p-corr\n",
      "        0   Neuroticism   Extraversion -0.325   0.000\n",
      "        1   Neuroticism       Openness -0.027   0.543\n",
      "        2   Neuroticism  Agreeableness -0.150   0.002\n",
      "        3  Extraversion       Openness  0.240   0.000\n",
      "        4  Extraversion  Agreeableness  0.060   0.358\n",
      "        5      Openness  Agreeableness  0.173   0.000\n",
      "        \n",
      "        >>> # Display the raw p-values with four decimals\n",
      "        >>> df.iloc[:, [0, 1, 3]].rcorr(stars=False, decimals=4)\n",
      "                      Neuroticism Extraversion Agreeableness\n",
      "        Neuroticism             -       0.0000        0.0028\n",
      "        Extraversion      -0.3501            -        0.2305\n",
      "        Agreeableness      -0.134       0.0539             -\n",
      "        \n",
      "        >>> # With the sample size on the upper triangle instead of the p-values\n",
      "        >>> df.iloc[:, [0, 1, 2]].rcorr(upper='n')\n",
      "                     Neuroticism Extraversion Openness\n",
      "        Neuroticism            -          500      497\n",
      "        Extraversion       -0.35            -      497\n",
      "        Openness           -0.01        0.265        -\n",
      "    \n",
      "    rm_corr(data=None, x=None, y=None, subject=None)\n",
      "        Repeated measures correlation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : :py:class:`pandas.DataFrame`\n",
      "            Dataframe.\n",
      "        x, y : string\n",
      "            Name of columns in ``data`` containing the two dependent variables.\n",
      "        subject : string\n",
      "            Name of column in ``data`` containing the subject indicator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stats : :py:class:`pandas.DataFrame`\n",
      "        \n",
      "            * ``'r'``: Repeated measures correlation coefficient\n",
      "            * ``'dof'``: Degrees of freedom\n",
      "            * ``'pval'``: p-value\n",
      "            * ``'CI95'``: 95% parametric confidence intervals\n",
      "            * ``'power'``: achieved power of the test (= 1 - type II error).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        plot_rm_corr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Repeated measures correlation (rmcorr) is a statistical technique for determining the common\n",
      "        within-individual association for paired measures assessed on two or more occasions for\n",
      "        multiple individuals.\n",
      "        \n",
      "        From `Bakdash and Marusich (2017)\n",
      "        <https://doi.org/10.3389/fpsyg.2017.00456>`_:\n",
      "        \n",
      "            *Rmcorr accounts for non-independence among observations using analysis\n",
      "            of covariance (ANCOVA) to statistically adjust for inter-individual\n",
      "            variability. By removing measured variance between-participants,\n",
      "            rmcorr provides the best linear fit for each participant using parallel\n",
      "            regression lines (the same slope) with varying intercepts.\n",
      "            Like a Pearson correlation coefficient, the rmcorr coefficient\n",
      "            is bounded by − 1 to 1 and represents the strength of the linear\n",
      "            association between two variables.*\n",
      "        \n",
      "        Results have been tested against the `rmcorr <https://github.com/cran/rmcorr>`_ R package.\n",
      "        \n",
      "        Missing values are automatically removed from the dataframe (listwise deletion).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pingouin as pg\n",
      "        >>> df = pg.read_dataset('rm_corr')\n",
      "        >>> pg.rm_corr(data=df, x='pH', y='PacO2', subject='Subject')\n",
      "                       r  dof      pval           CI95%     power\n",
      "        rm_corr -0.50677   38  0.000847  [-0.71, -0.23]  0.929579\n",
      "        \n",
      "        Now plot using the :py:func:`pingouin.plot_rm_corr` function:\n",
      "        \n",
      "        .. plot::\n",
      "        \n",
      "            >>> import pingouin as pg\n",
      "            >>> df = pg.read_dataset('rm_corr')\n",
      "            >>> g = pg.plot_rm_corr(data=df, x='pH', y='PacO2', subject='Subject')\n",
      "\n",
      "DATA\n",
      "    __all__ = ['corr', 'partial_corr', 'pcorr', 'rcorr', 'rm_corr', 'dista...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\신주현\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pingouin\\correlation.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pg.correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5aa634c14859627405404137fecabd2d911e45a844039ce454abb52cf9f696dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
